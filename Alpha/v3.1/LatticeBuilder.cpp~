//#include "LatticeBuilder.hpp"
#include "Controller.hpp"

LatticeBuilder::LatticeBuilder()
{}

LatticeBuilder::LatticeBuilder(LayoutManager* layoutManagerPtr)
{
  controller = layoutManagerPtr;

  /*
  Initial state requirements: allocate lattice to sufficient size to be able to handle max possible cluster size and
  max likely clusters.  Of these, initialize all members to 0, but the first item in each cluster, init it to 65535 or
  other signal value.

  As we append new clusters to the lattice, the next column will by default have all zero entries except for the first
  item identified as 65535 (or other signal) and all transition probabilities set to 1.0. Or some similar scheme...
  */

  //TODO: resolve the memory reservation strategy once data model settles
/*
  //build the lattice columns
  lattice.reserve(MAX_COLS);
  lattice.resize(MAX_COLS);
  for(int i = 0; i < lattice.size(); i++){
    lattice.alphas.resize(MAX_CLUSTER_ALPHAS);  //resize each column to accomodate the max number of letters in a cluster
    for(int j = 0 ; j < MAX_CLUSTER_ALPHAS; j++){
      lattice.alphas[j].arcs.reserve(MAX_CLUSTER_ALPHAS);
    }
  }

  //init the lattice to all zero entries
  for(int i = 0; i < lattice.size(); i++){
    lattice.pReflexive = 0.0;  
    for(int j = 0; j < lattice.alphas.size(); j++){  //init all the alphas to zero
      lattice.alphas[j].pState = 0.0;
      lattice.alphas[j].symbol = (char)0;
    }
  }
 */
}

LatticeBuilder::~LatticeBuilder()
{
  //this is done by the language specification. stl data structure destructors are called recursively
}


//not a primary utility. this is just a lazy testing function, without any other context. inits arc size for every state of some lattice
void LatticeBuilder::InitArcSizes(Lattice& lattice)
{
  int i, j;

  for(i = 0; i < lattice.size() - 1; i++){
  //cout << "init state[" << i << "].alphas arcs to size " << lattice[i + 1].alphas.size() << endl;
    for(j = 0; j < lattice[i].alphas.size(); j++){
      lattice[i].alphas[j].arcs.resize( lattice[i + 1].alphas.size() );
    }
  }
}



//builds some test lattice with good test characteristics
void LatticeBuilder::TestBuildLattice(Lattice& lattice)
{
  //build a lattice of seven columns, equivalent to a seven-letter word: 'seventh'
  Cluster c1, c2, c3, c4, c5, c6, c7;

  //build c1
  vector<char> neighbors = {'a','w','e','d','x','z','s'};
  c1.alphas.resize(neighbors.size());
  for(int i = 0; i < c1.alphas.size(); i++){
    c1.alphas[i].symbol = neighbors[i];
    c1.alphas[i].maxPrev = NULL;
  }
  //enumerate the internal probabilities
  c1.alphas[0].pState = (-1.0) * log2(0.1);
  c1.alphas[1].pState = (-1.0) * log2(0.05);
  c1.alphas[2].pState = (-1.0) * log2(0.05);
  c1.alphas[3].pState = (-1.0) * log2(0.2);
  c1.alphas[4].pState = (-1.0) * log2(0.1);
  c1.alphas[5].pState = (-1.0) * log2(0.3);  
  c1.alphas[6].pState = (-1.0) * log2(0.2);    //damage
  for(int i = 0; i < c1.alphas.size(); i++){
    c1.alphas[i].viterbiMax = ZERO_LOG_PROB;
    c1.alphas[i].maxPrev = NULL;
  }
  c1.pReflexive = (-1.0) * log2(0.2);

  //build c2
  neighbors = {'w','s','d','r','e'};
  c2.alphas.resize(neighbors.size());
  for(int i = 0; i < c2.alphas.size(); i++){
    c2.alphas[i].symbol = neighbors[i];
    c2.alphas[i].maxPrev = NULL;
  }
  //enumerate the internal probabilities
  c2.alphas[0].pState = (-1.0) * log2(0.1);
  c2.alphas[1].pState = (-1.0) * log2(0.2);
  c2.alphas[2].pState = (-1.0) * log2(0.2);
  c2.alphas[3].pState = (-1.0) * log2(0.3);  //bad noise
  c2.alphas[4].pState = (-1.0) * log2(0.2);
  for(int i = 0; i < c2.alphas.size(); i++){
    c2.alphas[i].viterbiMax = ZERO_LOG_PROB;
    c2.alphas[i].maxPrev = NULL;
  }
  c2.pReflexive = (-1.0) * log2(0.2);

  //build c3
  neighbors = {'c','f','g','b','v'};
  c3.alphas.resize(neighbors.size());
  for(int i = 0; i < c3.alphas.size(); i++){
    c3.alphas[i].symbol = neighbors[i];
    c3.alphas[i].maxPrev = NULL;
  }
  //enumerate the internal probabilities
  c3.alphas[0].pState = (-1.0) * log2(0.2);
  c3.alphas[1].pState = (-1.0) * log2(0.2);
  c3.alphas[2].pState = (-1.0) * log2(0.1);
  c3.alphas[3].pState = (-1.0) * log2(0.15);
  c3.alphas[4].pState = (-1.0) * log2(0.35);
  for(int i = 0; i < c3.alphas.size(); i++){
    c3.alphas[i].viterbiMax = ZERO_LOG_PROB;
    c3.alphas[i].maxPrev = NULL;
  }
  c3.pReflexive = (-1.0) * log2(0.2);

  //build c4
  neighbors = {'w','s','d','r','e'};
  c4.alphas.resize(neighbors.size());
  for(int i = 0; i < c4.alphas.size(); i++){
    c4.alphas[i].symbol = neighbors[i];
    c4.alphas[i].maxPrev = NULL;
  }
  //enumerate the internal probabilities
  c4.alphas[0].pState = (-1.0) * log2(0.1);
  c4.alphas[1].pState = (-1.0) * log2(0.1);
  c4.alphas[2].pState = (-1.0) * log2(0.3);
  c4.alphas[3].pState = (-1.0) * log2(0.1);
  c4.alphas[4].pState = (-1.0) * log2(0.4);
  for(int i = 0; i < c4.alphas.size(); i++){
    c4.alphas[i].viterbiMax = ZERO_LOG_PROB;
    c4.alphas[i].maxPrev = NULL;
  }
  c4.pReflexive = (-1.0) * log2(0.2);

  //build c5
  neighbors = {'b','h','j','m','n'};
  c5.alphas.resize(neighbors.size());
  for(int i = 0; i < c5.alphas.size(); i++){
    c5.alphas[i].symbol = neighbors[i];
    c5.alphas[i].maxPrev = NULL;
  }
  //enumerate the internal probabilities
  c5.alphas[0].pState = (-1.0) * log2(0.1);
  c5.alphas[1].pState = (-1.0) * log2(0.2);
  c5.alphas[2].pState = (-1.0) * log2(0.1);
  c5.alphas[3].pState = (-1.0) * log2(0.2);
  c5.alphas[4].pState = (-1.0) * log2(0.4);
  for(int i = 0; i < c5.alphas.size(); i++){
    c5.alphas[i].viterbiMax = ZERO_LOG_PROB;
    c5.alphas[i].maxPrev = NULL;
  }
  c5.pReflexive = (-1.0) * log2(0.2);

  //build c6
  neighbors = {'r','f','g','y','t'};
  c6.alphas.resize(neighbors.size());
  for(int i = 0; i < c6.alphas.size(); i++){
    c6.alphas[i].symbol = neighbors[i];
    c6.alphas[i].maxPrev = NULL;
  }
  //enumerate the internal probabilities
  c6.alphas[0].pState = (-1.0) * log2(0.1);
  c6.alphas[1].pState = (-1.0) * log2(0.1);
  c6.alphas[2].pState = (-1.0) * log2(0.2);
  c6.alphas[3].pState = (-1.0) * log2(0.1);
  c6.alphas[4].pState = (-1.0) * log2(0.6);
  for(int i = 0; i < c6.alphas.size(); i++){
    c6.alphas[i].viterbiMax = ZERO_LOG_PROB;
    c6.alphas[i].maxPrev = NULL;
  }
  c6.pReflexive = (-1.0) * log2(0.2);

  //build c7
  neighbors = {'g','y','u','j','n','b','h'};
  c7.alphas.resize(neighbors.size());
  for(int i = 0; i < c7.alphas.size(); i++){
    c7.alphas[i].symbol = neighbors[i];
    c7.alphas[i].maxPrev = NULL;
  }
  //enumerate the internal probabilities
  c7.alphas[0].pState = (-1.0) * log2(0.2);
  c7.alphas[1].pState = (-1.0) * log2(0.2);
  c7.alphas[2].pState = (-1.0) * log2(0.05);
  c7.alphas[3].pState = (-1.0) * log2(0.05);
  c7.alphas[4].pState = (-1.0) * log2(0.05);
  c7.alphas[5].pState = (-1.0) * log2(0.05);
  c7.alphas[6].pState = (-1.0) * log2(0.4);
  for(int i = 0; i < c7.alphas.size(); i++){
    c7.alphas[i].viterbiMax = ZERO_LOG_PROB;
    c7.alphas[i].maxPrev = NULL;
  }
  c7.pReflexive = (-1.0) * log2(0.2);

  lattice.push_back(c1);
  lattice.push_back(c2);
  lattice.push_back(c3);
  lattice.push_back(c4);
  lattice.push_back(c5);
  lattice.push_back(c6);
  lattice.push_back(c7);

  InitArcSizes(lattice);

  //cout << "Lattice before BuildTransitionModel() ..." << endl;
  //PrintLattice(lattice);

  BuildTransitionModel(lattice);

  //cout << "Lattice after BuildTransitionModel() ..." << endl;
  //PrintLattice(lattice);

}

/*
  Clears the Lattice.  This function is more important than it appears, since for every word of input,
  we need to build and destroy a lattice model. In itself, this may also change; for instance, if a static
  lattice is initialized and used, instead of destroying/reallocating every word. The streaming/threaded program model tends to
  favor dynamic behavior, so the internal memory state of the lattice is always known.

  TODO: Requirements for this function may change accordingly when memory allocation statregies and data-models settle.
*/
void LatticeBuilder::ClearLattice(Lattice& lattice)
{
  lattice.clear();  //all internal destructors will be called: vector<State>, State.vector<arc>, etc.
}

void LatticeBuilder::PrintLattice(Lattice& lattice)
{
  int i, row, col, maxht;

  maxht = 0;
  //prints by row, not column; start by getting max height
  for(i = 0; i < lattice.size(); i++){
    if(maxht < lattice[i].alphas.size()){
      maxht = lattice[i].alphas.size();
      //cout << "new size is: " << lattice[i].alphas.size() << endl;
    }
      //cout << "size is: " << lattice[i].alphas.size() << endl;
  }

  printf("Current lattice:\n");
  //print by row
  for(row = 0; row < maxht; row++){
    for(col = 0; col < lattice.size(); col++){ //iterate columns of this row
      if(lattice[col].alphas.size() > row){
        printf("[%c|%2.2f]  ",lattice[col].alphas[row].symbol,lattice[col].alphas[row].pState);
      }
      else{
        printf("          ");
      }
    }
    printf("\n");
  }
}

/*
//TODO some other class needs to claim this function
void PrintLattice(const Lattice& lattice)
{
  int i, j, k, maxht;
  bool hit;

  maxht = 0;
  for(i = 0; i < lattice.size(); i++){
    if(maxht < lattice[i].alphas.size()){
      maxht = lattice[i].alphas.size();
    }
  }

  //prints the vertices in columnar format
  printf("Lattice vertices:\n");
  i = 0; hit = false;
  while(i < maxht){
    for(j = 0; j < lattice.size(); j++){
      if(lattice[j].alphas.size() > i){
        printf("%c %3.1f   ", lattice[j].alphas[i].symbol, lattice[j].alphas[i].pState);
      }
      else{
        printf("        ");
      }
    }
    printf("\n");
    i++;
  }

  //print the edges
  for(i = 0; i < lattice.size() - 1; i++){
    
    printf("Column %d\n",i);
    for(j = 0; j < lattice[i].alphas.size(); j++){
      printf("  State %c: ",lattice[i].alphas[j].symbol);
      for(k = 0; k < lattice[i].alphas[j].arcs.size(); k++){
        if(lattice[i].alphas[j].arcs[k].dest != NULL){
          printf("(%c,%2.1f) ",lattice[i].alphas[j].arcs[k].dest->symbol,lattice[i].alphas[j].arcs[k].pArc);
        }
        else{
          printf("(NULL) ");
        }
      }
      printf("\n");
    }
  }
}
*/

/*
  The primary responsibility of this class. Build the lattice, and assign
  conditional probabilities based on physical distances.

  This is the static, non-streaming version of lattice-building.

  Secondary responsibility is re-conditioning the probabilities based on character n-gram probabilities.
*/
void LatticeBuilder::BuildStaticLattice(vector<PointMu>& pointMeans, Lattice& lattice)
{
  cout << "in BuildStaticLattice(), inData.size()=" << pointMeans.size() << endl;

  //for each point in inData, map it to a collection of immediate neighbors (keys), each with a probability with respect to its distance from the point
  for(int i = 0; i < pointMeans.size(); i++){
    cout << "appending mean: <" << pointMeans[i].pt.X << "," << pointMeans[i].pt.Y << ">" << endl;
    AppendCluster(pointMeans[i], lattice);
    //these types are wrong. this should be some graph/lattice type
    //SearchForNeighborKeys(inData[i],neighbors);  //gets all the neighbors and assigns physical probabilities to each one
    //lattice.push_back(neighbors);
  }
  PrintLattice(lattice);
}

/*
  Given some raw 'tick' value of the number of ticks spent in a given cluster,
  map this to a likelihood for a reflexive transition in that state. For instance,
  if the user focuses on 's' for longer, we would like to know if that indicates 'ss' as input,
  (or possibly some close neighbors of 's', whose transition can't be detected with current system resolution)

  Returns: a real number indicating the likelihood of a reflexive transition. When modelling this numerically, think of 
  the values with which this one is competing: the forward edges of the lattice. This parameter must be able to compete
  with these values (in terms of magnitude), if it is to have any use.


  TODO: define this more function's behavior once testing begins. It is a precision parameter.

*/
double LatticeBuilder::CalculateReflexiveLikelihood(U16 ticks)
{
  //for now, this is a simple threshold function: exceed threshold, and trigger. This is a discrete strategy,
  //but one that is likely to work with little calibration.
  if(ticks >= REFLEXIVE_TICK_THRESHOLD){
    return 0.5;
  }
  
  return 0.0;
}

//Comparison function for sorting arc's by probability, in ascending order, min item first. We're in -log2-space, so minimization is desired.
bool LatticeBuilder::MaxArcLlikelihood(const Arc& left, const Arc& right)
{
  return left.pArc < right.pArc;
}

void LatticeBuilder::SetLayoutManager(LayoutManager* layoutManagerPtr)
{
  layoutManager = layoutManagerPtr;
}


/*
  Initializes a cluster in the lattice; caller will then append the cluster.
  Given a point-mean, appends a cluster of neighbors around that point, each with a geometry-base probability.
*/
void LatticeBuilder::InitCluster(Cluster& newCluster, PointMu& mu)
{
  int i;
  double dist, maxPrev, normal = 0.0;
  Point tempPt;
  //lookup nearest neighbors
  char nearest = layoutManager->FindNearestKey(mu.pt);
  cout << "nearest=" << nearest << endl;
  vector<char>* neighbors = layoutManager->GetNeighborPtr(nearest);

  //get the likelihood of a reflexive arc on this cluster
  newCluster.pReflexive = CalculateReflexiveLikelihood(mu.ticks);

  newCluster.alphas.resize(neighbors->size() + 1);  //add one to account for the nearest key (var "index") itself
  for(i = 0; i < neighbors->size(); i++){ //from 1, since alphas isn't 

    newCluster.alphas[i].symbol = (*neighbors)[i];
    newCluster.alphas[i].viterbiMax = 0.0;
    newCluster.alphas[i].maxPrev = NULL;

    tempPt = layoutManager->GetPoint(newCluster.alphas[i].symbol);
    newCluster.alphas[i].pState = 1.0 / layoutManager->DoubleDistance(mu.pt,tempPt); //init every state to its real distance to the mean-point mu
    normal += newCluster.alphas[i].pState;
  }
  //init the nearest key, which by this procedure will always be last in the vector
  newCluster.alphas[newCluster.alphas.size()-1].viterbiMax = 0.0;
  newCluster.alphas[newCluster.alphas.size()-1].maxPrev = NULL;
  newCluster.alphas[newCluster.alphas.size()-1].symbol = nearest;
  tempPt = layoutManager->GetPoint(nearest);
  dist = layoutManager->DoubleDistance(mu.pt, tempPt);
  if(dist > 1.0){
    newCluster.alphas[newCluster.alphas.size()-1].pState = 1.0 / dist;
  }
  else{  //a bullseye; this is div-zero protection.
    newCluster.alphas[newCluster.alphas.size()-1].pState = 1.0;
  }
  normal += newCluster.alphas[newCluster.alphas.size()-1].pState;

  //now init the state probabilities by simple normalization
  //TODO: define this geometric probability better. It would probably benefit from being biased toward the mean point, and giving
  //      lower probability to farther keys. The method used is a parameter for our models; the best one needs to be determined through expt.
  //      Curvature of the prob-space should be center-biased; another benefit may be tracking the max distance, and definined this key as
  //      zero-probability.
  /*
  normal = 0.0;
  for(i = 0; i < newCluster.alphas.size(); i++){
    newCluster.alphas[i].pState = 1.0 / newCluster.alphas[i].pState;  //TODO: try other functions, such as conical or log-based prob-surfaces
    //newCluster.alphas[i].pState /= sumDist;  //TODO: try other functions, such as conical or log-based prob-surfaces
    normal += newCluster.alphas[i].pState;
    //newCluster.alphas[i].pState = (-1.0) * log2(newCluster.alphas[i].pState);
  }
  */
  for(i = 0; i < newCluster.alphas.size(); i++){
    newCluster.alphas[i].pState = (-1.0) * log2(newCluster.alphas[i].pState / normal);
  }
}

/*
  Given an input (mu, a mean point), append a new cluster.
  Could search for nearest neighbor points in real-space. Currently,
  I think it is sufficient to identify the nearest key, and grab its neighbors based
  on keyboard layout.  

  Input: a point mean, representing a spatial mean point along with the estimate time spent their (which gives the likelihood
  a reflexive arc occurred for this cluster).

*/
void LatticeBuilder::AppendCluster(PointMu& mu, Lattice& lattice)
{
  int i, j, prevCol;

  //get nearest neighbor keys of point, via pre-defined static key-clusters
  //TODO: evaluate if static clusters should be used, or continuous value ones. For instance, should 's' alwasy return {aqwedxz} regardless of the focus on 's' key
  Cluster newCluster;
  lattice.push_back(newCluster); //i kinda prefer push_back (dynamic addition), as opposed to static=; likely better for streaming
  InitCluster(lattice[lattice.size()-1],mu);

  //Each time a cluster is appended, resize the arc vectors in the previous cluster each to the number of states in this cluster.
  //But only do so if there is a previous cluster (eg, this is not the start state).
  //The edges are initialized with the probability of the target state.
  if(lattice.size() > 1){
    prevCol = lattice.size() - 2; //get the previous column in the lattice
    for(i = 0; i < lattice[prevCol].alphas.size(); i++){
      for(j = 0; j < newCluster.alphas.size(); j++){
        //Arc newArc = {&newCluster.alphas[j], newCluster.alphas[j].pState};
        lattice[prevCol].alphas[i].arcs.push_back(Arc {&newCluster.alphas[j], newCluster.alphas[j].pState});
      }
    }
  }
  else{
    //for the first added column, init the back pointers to null, the maxProb value to a infinite flag val
    for(i = 0; i < newCluster.alphas.size(); i++){
      newCluster.alphas[i].maxPrev = NULL;
      newCluster.alphas[i].viterbiMax = INIT_STATE_LOG_PROB;
    }
  }
}

/* 
  Once---or possibly in streaming fashion, once a new cluster has been appended--

  Try to write this to encompass both states of the lattice: streaming and static lattice models.

  Obviously each arc is composed of a source and a dest, both of which are alphas (letters/keys). Each alpha
  has a unique index, so each src/dest is an alpha/index pair of type char/int
  
  Pre-condition: Each state's arc vector has been appropriately sized. When each cluster is appended, we rely
  on the arcs to be properly sized at initialization, not here. But this is also dependent on hazy streaming/static
  requirements.

  Notes: Only builds the extrinsic arcs, does not absorb or otherwise mix probability parameters. Viterbi will do that stuff.
  Watch how conditioning or other features are applied; using bigrams is okay, but trigrams and above breaks dynamic programming
  behavior. See Jurafsky SLP Ch. 10. Just things to remember.

  REVISION: I revised this to assign only internal state probabilities to the arcs. N-gram data will no longer be applied to the
  graph, but instead only to the set of candidate paths it generates. See .hpp for comments about no longer using n-gram data.

  Notes: Since we're no longer using arc-conditioning data, storing the arc-probabilities in the arcs themselves is actually redundant, 
  since we could instead just get the target state's probability while searching. But I'm leaving this in for now, since maybe in the
  future some other conditioning data on the arcs could be used.

*/
void LatticeBuilder::BuildTransitionModel(Lattice& lattice)
{
  int i, j, k;

  /*
    Starting at the 0th index in lattice, the arcs denote the transition from current alpha to some alpha in next column of the lattice.
    The last column in the matrix is the end of the word, so there is no next column to which to assign arcs. The <stop> state is simply
    implied by column size, not flagged anywhere, so don't rely on anything other than the lattice width/size.
  */

  //iterate up to n-1 columns of the lattice, assigning arcs for every alpha to every alpha in next column. Therefore if columns are n, there will be n^2 arcs.
  for(i = 0 ; i < lattice.size() - 1; i++){
  
    //assume there's a problem if first state's arc != n_states in next col, and resize all arcs in current column to match n_states
    if(lattice[i].alphas[0].arcs.size() != lattice[i+1].alphas.size()){
      cout << "WARN resizing arc count for current state, which should have been initialized correctly. In BuildTransitionModel()" << endl;
      for(j = 0; j < lattice[i].alphas.size(); j++){
        lattice[i].alphas[j].arcs.resize(lattice[i + 1].alphas.size());
      }
    }

    //foreach alpha in current column, assign each of its arcs an edge weight given by an pState of dest-state
    for(j = 0; j < lattice[i].alphas.size(); j++){
      for(k = 0; k < lattice[i+1].alphas.size(); k++){
        //init the next_state pointer
        lattice[i].alphas[j].arcs[k].dest = &lattice[i + 1].alphas[k];
        //init the log-probability of following that pointer
        lattice[i].alphas[j].arcs[k].pArc = lattice[i + 1].alphas[k].pState;

        /*  OBSOLETE
        if(USE_NGRAM_DATA){
          lattice[i].alphas[j].arcs[k].pArc += BIGRAM_LAMBDA * GetBigramProbability(lattice[i].alphas[j].symbol, lattice[i + 1].alphas[k].symbol); //each arc's probability is given by p(B|A) for path from state A to state B
          lattice[i].alphas[j].arcs[k].pArc += UNIGRAM_LAMBDA * GetUnigramProbability(lattice[i + 1].alphas[k].symbol); //each arc's probability is given by p(B|A) for path from state A to state B
          lattice[i].alphas[j].arcs[k].pArc *= NGRAM_MODEL_WEIGHT;
          //cout << "appended arc (" << lattice[i].alphas[j].symbol << i << "->" << lattice[i].alphas[j].arcs[k].dest->symbol << (i+1) << "," << lattice[i].alphas[j].arcs[k].pArc << ")" << endl;
        }
        else{
          lattice[i].alphas[j].arcs[k].pArc = 0.0; //effectively initial arc weight becomes a dont-care if set to 1.0
        }
        */
      }
    }
  }
  
}




/*

  Re-condition arcs based on n-gram probabilities (likely bi-gram data will be sufficient)

void LatticeBuilder::ConditionArcs(Lattice& lattice)
{
  //let p(arc_physical) be the physically-assigned probability based solely on location in relation to neighboring keys

  //for arc in lattice:
  // p(arc) = p(arc_physical|alpha,beta)
  // giving:
  // p(arc) = p(arc_physical)*p(alpha|beta)
}
*/




